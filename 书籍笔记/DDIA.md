# 1 数据系统基础

## 1.1 数据系统基本性质

数据系统主要特性有三点，分别是可靠性、可扩展性以及可维护性

* 可靠性：当出现意外情况，如软硬件故障等问题时，系统仍然能够保证功能正确。

* 可扩展性：即使当前系统运行正常，在未来，也有可能运转出现问题，典型的例子是负载增加。因此可扩展性指的是当系统负载的某些方面以某种方式增加时，系统有着什么样的应对措施。讨论的两个方面通常为：

  * 垂直拓展：增加单机的性能
  * 水平拓展：将负载分散到更多更小的机器当中

  需要注意，可扩展性，通常会对系统请求模式做出某种假设，然后对假设的情况做出特定的优化，因此对于假设错误的系统，或者初创公司尚未定型的产品，讨论可扩展性意义不大。

* 可维护性

## 1.2 数据存储与检索

* 日志：简单的追加结构，插入时可以快速的在尾部插入，查找必须从头到尾做查找。

日志这种线性结构，效率很低，为了更高效的进行查询，有着各种数据库构成的索引来进行加速。

* 哈希索引：也就是 hashmap。比较高效，可以通过 key-value(记录内容偏移)实现高效的查询等功能，但是 hashmap 必须存在内存中(磁盘上效率低)，那么有大量 key 的时候，就不是很适用了。
* 排序字符串表(SSTables)：将文件内容的 key 按大小进行排序。这样在合并时，可以高效的进行多路归并，而且索引可以利用稀疏索引来确定查找目标所在范围。这种引擎的工作流程如下：
  * 写入时，添加到内存中的有序结构(AVL/RBTree)
  * 内存大小达到阈值后，存入磁盘，新写入的操作，可以写入到一个新的内存表实例
  * 读请求，按照新旧顺序进行查找
  * 后台进程周期性的进行合并与压缩
* B-trees：其实就是多叉平衡树，因为多叉可以很大程度的降低树的高度所以很适合硬盘的结构。实际中，分支个数取决于存储页面的大小和内容大小。实际中，为了提供 crash-safe 的能力，都是通过 write-ahead log 操作，先更新日志，再对页内容进行更新。

数据系统通常被分为两类，一类是在线事务处理(OLTP)，涉及少量键值，根据用户操作更新数据，另一类是在线分析处理(OLAP)，涉及大量数据，以便进行数据分析。因为 OLTP 系统是交互式的，因此对延迟要求较高，在上运行分析系统可能会影响系统，因此通常单独的数据库做位 OLAP，也就是数据仓库。

# 2 分布式数据系统

部署分布式系统通常出于以下几个目的：扩展性（复制+分区）、容错与高可用行、延迟考虑。

## 2.1 数据复制

复制顾名思义，指的是多台机器储存着相同的副本，通常有以下几个目的：

* 延迟，让数据在地理上更接近用户，来降低延迟（公司多地机房，就有这方面的考虑）
* 可用性，当部分组件出现故障时，系统仍然能正常运行（raft 就是这个目的）
* 吞吐量，将请求打散到多台机器，提高整体吞吐量

### 2.1.1 主从复制

当有多个副本存在时，主从复制将副本分为一个主副本，和一个从副本，所有的写请求，必须写入主副本，再由主副本同步给从副本。

既然主节点需要做同步，那么就有着同步和异步的区别：

* 同步复制：主节点等待从节点更新成功后，再返回给客户端。好处显然是主从节点数据一致性好，坏处是增大了客户端响应延迟。
* 异步复制：主节点不必等待从节点更新完成，发送消息给从节点后，即可直接返回。

二者可以混用，实际中，通常是某一个从节点是同步的，而其他节点是异步的。对于新的从节点，通常采用快照+日志的方式进行追赶(redis 就是)。

复制技术：

* 基于语句：将主节点执行的每个请求，直接发送给从节点。但是有一些问题：
  * 某些语句，如 now，rand 一类的不确定函数，可能再不同副本产生不同值。（可以通过替换为主节点确定的值）
  * 如果语句执行结果依赖现有数据，则必须所有副本执行的顺序一致，才能使数据一致
* 基于 WAL 日志：使用记录了磁盘写入信息的日志来复制，这样会得到完全相同的副本。
  * WAL 相关日志记录的信息非常底层，使得复制方案和存储引擎紧耦合
* 基于逻辑日志：类似 binlog，记录了描述表行级别的写请求。

为了提高请求吞吐量，常常把请求打散到各个节点，那么那些异步的从节点，就有着数据不一致的问题，通常有以下几个解决方式：

* 读自己的写：因为用户通常希望自己修改完，能够立马读取到自己的更改，具体有几种方式
  * 访问可能会被修改的内容，就从主节点，否则从从节点。（比如用户读取他人主页，这种就一定不会被自己更改）
  * 如果应用可能被大部分用户更改，就需要其他方式，比如限制条目更新后的一分钟内，只能从主节点读取
* 单调读：用户的多个请求，可能被打散到多个从节点上，那么可能出现多次请求不一致的问题。单调度就是保证用户进行多次读取，不会看到回滚现象，也就是读取时间戳要大于等于上一次的。

### 2.1.2 多主节点复制

在单个数据中心内部设立多个主节点，意义不大，但是当有多个数据中心时，就可以降低应用的延迟，并且容忍数据中心失效，但是多主节点会带来一系列的问题：

* 写冲突：多个用户同时想不同的主节点发起修改，会发生冲突。通常采用的解决方式是避免冲突，例如对于某一用户，总是路由到特定的数据中心
* 收敛于一致状态：有时候不可避免需要在多个主节点更新信息，那么由于多主节点间，没有明确的先后关系，所以难以确定数据的最终状态，有几个解决思路：
  * 给每个写请求分配一个 id，比如时间戳，让 id 作为写入的逻辑时钟，id 最高的写入成功。
  * 给每个副本指定 id，比如让某个数据中心的写入，优于其他的数据中心。

### 2.1.3 无主节点复制

。。。。。基本就是 raft 那一套东西，先不记了

## 2.2 数据分区

有时候我们的数据大到单机根本存不下，或者查询压力非常高的情况下，光靠复制就解决不了问题了，这时候就得靠分区(sharding)。

### 2.2.1 分区方法

* 关键字区间分区：如果知道关键字的上下限，可以对范围进行切分后，分布到各个分区。考虑数据访问特性，区间段并不一定要均匀，分区的目的是尽量保证各个分区的访问量尽可能的均匀。
  * 关键字区间的缺点在于很有可能导致热点访问，比如以时间作为 key，那在某天写入的时候，会集中写入到一个点。可以通过联合关键字区间来尝试解决
* 关键字哈希分区：最常用且好用的避免热点的问题，是用哈希函数来打散各个关键字。
  * 哈希虽然提供了很好的均匀分布特性，但是想要做到区间查询的话，就必须在所有分区进行。

### 2.2.2 分区再平衡

分区后，随着规模增大，或者节点失效，不可避免的需要重新分配各个机器上的分区，我们可以：

* 取模：方法简单，例如有 n 个节点，直接将 key 按照 hash(key)  mod n 分配节点。这个方法的缺陷很大，因为当节点个数改变的时候，需要迁移非常多的关键字，时间成本很大。
* 超大数量的固定数量分区：比如预计有 10 个左右的节点，但是分成 10000 这种，远超节点个数的分区数，那么当需要重平衡的时候，比如新增节点，那么只需要从现有的节点中，分出一些分区给新节点即可平衡。

### 2.2.3 请求路由





